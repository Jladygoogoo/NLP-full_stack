{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - torchtext的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然在输入模型前，数据处理的基本思路都是分词、清洗、数学表示，但是神经网络对于数据结构的要求和传统机器学习不太一样，主要表现为神经网络在采取`批量梯度下降法`(BGD)时，需要小批量地输入数据，同时其中对`训练/验证/测试集`概念使用的也更加频繁。对此，pytorch中的`torchtext`库提供了一套**完整**的文本数据集处理机制，使用起来非常方便，而且也可以应用于除pytorch外的框架中（如tensorflow/keras）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "几个核心概念：\n",
    "+ **Field**：`域`，一步解决`分词、清洗、表示`的问题\n",
    "+ **Dataset**：`数据集`，读取数据文件，可进行split操作\n",
    "+ **Iterator**：`迭代器`，由dataset得到，为最终输入，同样可以进行split操作\n",
    "\n",
    "数据流：\n",
    "\n",
    "    data -> Dataset -> Dataloader/Iterator\n",
    "\n",
    "基本过程：\n",
    "1. 定义Field实例对象\n",
    "2. 利用数据生成examples得到Dataset，该过程调用Field.preprocess，包括：tokenize -> pipeline -> preprocessing\n",
    "3. 基于Dataset构建vocab词典（build_vocab），可以传入词的向量表示（vectors）\n",
    "4. 基于Dataset得到Iterator，在构建batch的过程中调用Field.process，包括：pad -> numericalize -> tensor（最终产物）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import traceback\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torchtext import data, vocab\n",
    "from torchtext.data import Dataset, TabularDataset\n",
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field\n",
    "以下是构建`Field`实例对象时可传入的所有参数：\n",
    "\n",
    "   **Field**(**sequential**=True, **use_vocab**=True, init_token=None, eos_token=None, **fix_length**=None, **dtype**=torch.int64, **preprocessing**=None, postprocessing=None, **lower**=False, **tokenize**=None, tokenizer_language='en', include_lengths=False, **batch_first**=False, pad_token='<pad>', unk_token='<unk>', pad_first=False, truncate_first=False, stop_words=None, is_target=False)\n",
    "\n",
    "其中：\n",
    "+ **sequential**：指单条数据是否是序列的（如一句话可看作一个序列），为`label`构建Field时将其指定为**False**\n",
    "+ **use_vocab**：如果label数据已经是整形，则设置use_vocab=False\n",
    "+ **fix_length**：是否使用padding/cut指定序列长度\n",
    "+ **preprocessing**：`\"The Pipeline that will be applied to examples using this field after tokenizing but before numericalizing\"`. -> 可传入一个preprocessor，介于分词与表示之间，用于`清洗等`其他操作\n",
    "+ **lower**：`str.lower()`\n",
    "+ **tokenize**：传入分词器\n",
    "+ **batch_first**：是否将batch作为数据的第一个维度\n",
    "+ **init_token/eos_token/pad_token/unk_token**：特殊符号的表示\n",
    "    \n",
    "Field定义了如何处理数据，其中两个核心方法为`preprocess`和`process`。注意Field只是**`规则`**的指定，只有当数据传入后才能构建实体词典。\n",
    "    \n",
    "如果要使用现成的模型，则需要沿用其Field的构建，具体方法参见*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r',|\\.|\\?|!','',text)\n",
    "    tokens = text.split()\n",
    "    tokens = list(filter(lambda x:len(x)>0,tokens))\n",
    "    if len(tokens)<5:\n",
    "        tokens.extend([' ']*(5-len(tokens)))\n",
    "    return tokens\n",
    "\n",
    "# 设定text和label的Field\n",
    "text_field = data.Field(lower=True,tokenize=tokenizer,batch_first=True)\n",
    "label_field = data.Field(sequential=False,batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "+ Dataset由examples组成【examples必须是data.Example类型】\n",
    "+ 一个example由`text和label`组成，分别对应data中的X和y。\n",
    "+ X->text, 以及y->label要经过`Field`的处理（映射）\n",
    "+ data.Dataset是终极父类，如果要`自定义dataset`从它`继承`\n",
    "+ data.`TabularDataset`(继承类)可以用来读取存储在\"CSV\"，\"TSV\"或\"JSON\"中的数据\n",
    "+ dataset提供`splits类方法`用于同时生成train/validation/test数据的dataset，但是需要指定各自的文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = TabularDataset.splits(\n",
    "        path='processed_data', format='csv', skip_header=True,\n",
    "        train='best3_train.csv', test='best3_test.csv',\n",
    "        fields=[('text',text_field),('label',label_field)])\n",
    "# 注意！\n",
    "# fields=[f1,f2,...]严格按照csv中的列顺序，'text'和'label'是对每列数据重新赋予的名字\n",
    "# 在传入前应该先将index列除去"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json或许是比csv/tcs更好的选择，因为：1.可以存储列表 2.无需担心tab等符号使得列读取混乱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build_vocab\n",
    "根据具体的数据集构建Vocab对象 -> `Field.build_vocab` -> `Field.vocab`\n",
    "\n",
    "Field.vocab对象中，如果不指定vectors，则依据词频构建词典（得到vocab.itos\\/stoi）。否则使用指定的vectors。\n",
    "\n",
    "vocab.vectors的构造有两种方式：\n",
    "1. 利用库中自带的向量模型\n",
    "\n",
    "  可选的预训练模型有`\"charngram.100d\", \"fasttext.en.300d\", \"fasttext.simple.300d\", \"glove.42B.300d\", \"glove.840B.300d\", \"glove.twitter.27B.25d\", \"glove.twitter.27B.50d\", \"glove.twitter.27B.100d\", \"glove.twitter.27B.200d\", \"glove.6B.50d\", \"glove.6B.100d\", \"glove.6B.200d\", \"glove.6B.300d\"`。其中d表示向量长度。\n",
    "\n",
    "2. 传入外部训练的向量模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.利用自带的pretrain模型，注意下载可能需要比较久的时间\n",
    "# text_field.build_vocab(train_dataset,test_dataset,vectors='glove.6B.200d')\n",
    "\n",
    "# 2.外部传入向量模型\n",
    "# vectors = vocab.Vectors(name=embedding_model_path,cache=cache_path)\n",
    "# name为传入模型的路径，cache为向量数据缓存的路径，默认在.vector_cache文件夹下\n",
    "\n",
    "# 此处直接使用索引而非向量\n",
    "text_field.build_vocab(train_dataset,test_dataset)\n",
    "label_field.build_vocab(train_dataset,test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实不相瞒，我从来没有下成功过。\n",
    "\n",
    "<img src='images/memes/RuntimeError.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator\n",
    "迭代器，用于生成batch。同样使用spilts方法来定义训练/验证/测试集。\n",
    "\n",
    "下面以BucketIterator为例，其与普通迭代器的区别在于，它会将长度相似的数据放在一个batch中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 62])\n",
      "torch.Size([64])\n",
      "tensor([    6, 10018,   393,     2,     2,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    datasets=(train_dataset,test_dataset),\n",
    "    sort_key=lambda x:len(x.text),shuffle=True,\n",
    "    batch_sizes=(64,int(len(test_dataset)/16)))\n",
    "\n",
    "# 至此，观察数据的内部排列方式\n",
    "for batch in train_iter:\n",
    "    feature, target = batch.text, batch.label\n",
    "    print(feature.shape)\n",
    "    print(target.shape)\n",
    "    print(feature[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "流程回顾：\n",
    "\n",
    "1. 定义Field实例对象\n",
    "2. 利用Dataset生成examples，该过程调用Field.preprocess，包括：tokenize -> pipeline -> preprocessing\n",
    "3. 基于Dataset构建vocab词典（build_vocab），可以传入词的向量表示（vectors）\n",
    "4. 基于Dataset得到Iterator，在构建batch的过程中调用Field.process，包括：pad -> numericalize -> tensor（最终产物）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
