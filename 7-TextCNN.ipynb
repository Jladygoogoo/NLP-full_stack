{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - TextCNN\n",
    "上一节梳理了如何利用pytorch中的torchtext包进行`文本数据处理`，这一节将介绍如何搭建并训练`深度学习模型`，以CNN为例处理分类任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本原理\n",
    "`卷积神经网络`起源于图像处理，其基本思路是以卷积窗口为单位沿着二维像素矩阵滑动，在每个窗口内执行卷积操作，从而捕捉相邻像素间的信息，并压缩每一维的长度。应用到文本数据中，则将每一句话看作一个矩阵，每一个词都用定长的向量表示。使用的`卷积核宽度和词向量长度一致`（即只在词之间进行滑动，而不会将词拆分，有点n-gram的意思），高度可以设置多个。新得到的矩阵经过最大池化层后输入全连阶层，最后进行分类。\n",
    "\n",
    "<img src='images/textcnn.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码实现\n",
    "pytorch建模可以分为以下几部分：\n",
    "+ 搭建模型：定义模型类，其中初始函数需要设置模型结构（嵌入层、卷积层、全连阶层...），forward函数用于执行一轮训练\n",
    "+ 定义train/test/predict函数\n",
    "+ 定义主模块：参数配置、数据导入、调用模型和函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "阅读时请一定注意代码中的注释【流过的泪没有人知道】\n",
    "\n",
    "<img src='images/memes/debugging.jpeg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, vocab\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import BucketIterator\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 搭建模型\n",
    "搭建模型的过程实际上就是将torch.nn中现有的模块进行`组装`。\n",
    "\n",
    "关键是一定要清楚每一阶段`数据`输入输出的`形状`以及`卷积核/池化核`的`大小`！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(TextCNN,self).__init__()\n",
    "        self.config = config\n",
    "        N = config.embed_num #嵌入空间中词的个数\n",
    "        D = config.embed_dim #词向量的维数\n",
    "        C = config.class_num #类别数\n",
    "        Ci = 1\n",
    "        Co = config.kernal_thick #每个卷积核的厚度，固定值\n",
    "        Ws = config.kernal_widths #每个(一维)卷积核的宽/窗口大小，有多个->列表\n",
    "\n",
    "        self.embed = nn.Embedding(N,D)\n",
    "        self.conv1ds = nn.ModuleList([nn.Conv1d(D,Co,W) for W in Ws])\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.fc1 = nn.Linear(Co*len(Ws),C)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.embed(x) #(batch_size, seq_length, embed_dim)\n",
    "        if self.config.static: \n",
    "            # Variable的requires_grad属性默认为False\n",
    "            x = autograd.Variable(x) \n",
    "        x = x.permute(0,2,1) #(batch_size, embed_dim, seq_length)\n",
    "        \n",
    "        x = [F.relu(conv(x)) for conv in self.conv1ds] #[(batch_size, Co, Wi)...]\n",
    "        \n",
    "        # 利用池化消除窗口大小不同带来的维度差异 -> [(batch_size, Co)]\n",
    "        # xi.size(2)表示xi第三个维度上的大小\n",
    "        x = [F.max_pool1d(xi,xi.size(2)).squeeze(2) for xi in x] \n",
    "        \n",
    "        # 将所有卷积核的输出拼接到一起 -> (batch_size, Co*len(Ws))\n",
    "        x = torch.cat(x,1) \n",
    "        x = self.dropout(x)\n",
    "        return self.fc1(x) # -> (batch_size, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train/evaluate/predict/save\n",
    "+ train: 训练指定个epoch，一共更新参数epoch_num\\*batch_num次（用steps来表示）。定期进行准确率报告与模型保存。\n",
    "+ evaluate: 用于评价模型，大体步骤与train一致\n",
    "+ predict: 利用模型进行分类\n",
    "+ save: 保存模型至本地，使用方法`torch.save`(model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的几点：\n",
    "1. 利用torchtext进行label的转化表示，得到的区间从1开始，但是模型输入要求从0开始->`target.sub_(1)`【带_的操作表示inplace】\n",
    "2. 注意每一步后要将optimizer的梯度设为0，防止效应累加\n",
    "3. 评价标准为accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_iter,test_iter,config):\n",
    "    # optimizer控制模型参数的变化\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=config.lr)\n",
    "    \n",
    "    #将self.training设置为True，不同模式下模型的反应策略不同\n",
    "    model.train() \n",
    "    \n",
    "    step = 0\n",
    "    best_acc = 0\n",
    "    for epoch in range(1,config.epochs+1):\n",
    "        stop_flag = 0\n",
    "        for batch in train_iter:\n",
    "            step += 1\n",
    "            feature, target = batch.text, batch.label\n",
    "            with torch.no_grad(): # 不计入逆向传播中\n",
    "                target.sub_(1) # target-1，要求从0开始（否则报错）\n",
    "            \n",
    "            # 实际上执行model.forward(feature)操作 -> (batch_size, C)\n",
    "            logit = model(feature) \n",
    "\n",
    "            loss = F.cross_entropy(logit,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 生成报告\n",
    "            if step % config.log_interval == 0:\n",
    "                # torch.max(logit,1)[1]取logit第二维中的最大值，即概率最高的类别\n",
    "                # view相当于reshape\n",
    "                corrects = (torch.max(logit,1)[1].view(target.size()).data == target.data).sum()\n",
    "                accuracy = 100.0 * corrects/batch.batch_size\n",
    "                # '\\r'实现覆盖输出\n",
    "                sys.stdout.write(f\"\\rStep[{step}] - loss: {loss.item():.6f} \\\n",
    "                    acc: {accuracy:.4f}%({corrects.item()}/{batch.batch_size})\")\n",
    "            \n",
    "            # 阶段测试、模型保存，并判断是否提前结束训练\n",
    "            if step % config.test_interval == 0:\n",
    "                test_acc = evaluate(model,test_iter)\n",
    "                if test_acc > best_acc:\n",
    "                    best_acc = test_acc\n",
    "                    last_step = step\n",
    "                    save(model, config.save_dir, 'best', step)\n",
    "                else:\n",
    "                    if step - last_step >= config.early_stop:\n",
    "                        print('early stop by {} steps.'.format(config.early_stop))\n",
    "                        stop_flag = 1\n",
    "                        break\n",
    "        if stop_flag: break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate时注意model.training模式的转换。\n",
    "\n",
    "train时accuracy的输出基于每一个batch，此时要考虑整个数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,data_iter):\n",
    "    # 将self.training设置为False\n",
    "    model.eval() \n",
    "    \n",
    "    corrects, avg_loss = 0,0\n",
    "    for batch in data_iter: #注意这里的batch表示多组evaluation，最后取平均即可\n",
    "        feature, target = batch.text, batch.label\n",
    "        with autograd.no_grad():\n",
    "            target.data.sub_(1)\n",
    "\n",
    "        logit = model(feature)\n",
    "        # reduction='sum'表示每个batch内部不取平均\n",
    "        loss = F.cross_entropy(logit,target,reduction='sum') \n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        corrects += (torch.max(logit,1)[1].view(target.size()).data \n",
    "                    == target.data).sum()\n",
    "\n",
    "    size = len(data_iter.dataset)\n",
    "    avg_loss /= size \n",
    "    accuracy = 100.0 * corrects/size \n",
    "    print(f\"\\nEvaluation - loss: {avg_loss:.6f} acc: {accuracy:.4f}%({corrects}/{size})\\n\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict首先要将输入文本进行预处理（和训练前的处理要一致）。\n",
    "\n",
    "另外注意之前的标签值自减1，最终输出时要加回来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text,model,text_field,label_field,vectors=None):\n",
    "    assert isinstance(text,str), \"plz use str object as input.\"\n",
    "    # 将self.training设置为False\n",
    "    model.eval()\n",
    "    \n",
    "    # 相当于生成一个example\n",
    "    text = text_field.preprocess(text)\n",
    "    # 相当于build_vocab\n",
    "    text = [[text_field.vocab.stoi[word] for word in text]]\n",
    "    x = torch.tensor(text) # -> (batch_size,seq_length), batch_size=1\n",
    "    \n",
    "    x = autograd.Variable(x)\n",
    "    output = model(x)\n",
    "    _, pred = torch.max(output,1)\n",
    "    # pred.item()等价于pred.data[0]\n",
    "    print(label_field.vocab.itos[pred.item()+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型保存，利用state_dict()进行全面的参数记录，重载后还可以继续训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model,save_dir,save_prefix,steps):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_path = f\"{save_dir}/{save_prefix}_steps_{steps}.pt\"\n",
    "    torch.save(model.state_dict(),save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config\n",
    "用于配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # data\n",
    "    shuffle = False # whether to shuffle data every epoch, de:False\n",
    "\n",
    "    # model\n",
    "    dropout = 0.5 # dropout rate, de:0.5\n",
    "    max_norm = 3.0 # l2 constraint of parameters, de:3.0\n",
    "    embed_dim = 128 # embedding(word vectors) dimension, de:128\n",
    "    kernal_thick = 100 # number of each kind of kernel, de:100\n",
    "    kernal_widths = [3,4,5] # different kernal sizes used for convolution, de:[3,4,5]\n",
    "    static = False # whether to fix embeddings during training, de:Flase\n",
    "    save_dir = 'model/TextCNN' # where to save the model\n",
    "\n",
    "    # training\n",
    "    lr = 0.001 # initial learning rate, de:0.001\n",
    "    epochs = 5 # number of epochs for train, de:5\n",
    "    batch_size = 96 # batch size for training, de:64\n",
    "    log_interval = 1 # how many steps to wait before logging training status, de:1\n",
    "    test_interval = 100 # how many steps to wait before testing, de:100\n",
    "    early_stop = 1000 # iteration numbers to stop without performance increasing, de:1000\n",
    "\n",
    "    # use\n",
    "    cuda = False # whether to use cuda, de:False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main\n",
    "主模块，实现数据预处理、模型的训练/调用/测试\n",
    "\n",
    "数据处理的方法和上一节讲的一样⬇️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "### 数据 ###\n",
    "# 自定义分词器\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r',|\\.|\\?|!','',text)\n",
    "    tokens = text.split()\n",
    "    tokens = list(filter(lambda x:len(x)>0,tokens))\n",
    "    if len(tokens)<5:\n",
    "        tokens.extend([' ']*(5-len(tokens)))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# 设定text和label的Field\n",
    "text_field = data.Field(lower=True,tokenize=tokenizer,batch_first=True)\n",
    "label_field = data.Field(sequential=False,batch_first=True)\n",
    "\n",
    "\n",
    "# 读取数据并创建train和test数据集\n",
    "data_path = 'processed_data'\n",
    "train_dataset, test_dataset = TabularDataset.splits(\n",
    "        path=data_path, format='csv', skip_header=True,\n",
    "        train='best3_train.csv', test='best3_test.csv',\n",
    "        fields=[('text',text_field),('label',label_field)]) # 按表格顺序来的!!\n",
    "\n",
    "\n",
    "text_field.build_vocab(train_dataset,test_dataset)\n",
    "label_field.build_vocab(train_dataset,test_dataset)\n",
    "# 注意保存field便于后续使用\n",
    "with open('model/text_field.txt','wb') as f:\n",
    "    pickle.dump(text_field, f)\n",
    "with open('model/label_field.txt','wb') as f:\n",
    "    pickle.dump(label_field, f)\n",
    "\n",
    "\n",
    "# 创建迭代器\n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    datasets=(train_dataset,test_dataset),\n",
    "    sort_key=lambda x:len(x.text),shuffle=True,\n",
    "    batch_sizes=(config.batch_size,int(len(test_dataset)/16)))\n",
    "# for batch on iterator -> (batch_size,seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据处理完毕，开始训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step[100] - loss: 0.833781                     acc: 61.4583%(59/96)\n",
      "Evaluation - loss: 0.846032 acc: 62.8110%(3383/5386)\n",
      "\n",
      "Step[200] - loss: 0.570657                     acc: 77.0833%(74/96)\n",
      "Evaluation - loss: 0.780924 acc: 66.0416%(3557/5386)\n",
      "\n",
      "Step[300] - loss: 0.421939                     acc: 83.3333%(80/96)\n",
      "Evaluation - loss: 0.757187 acc: 67.6012%(3641/5386)\n",
      "\n",
      "Step[400] - loss: 0.270577                     acc: 91.6667%(88/96)\n",
      "Evaluation - loss: 0.774350 acc: 68.5481%(3692/5386)\n",
      "\n",
      "Step[500] - loss: 0.348337                     acc: 91.6667%(88/96)\n",
      "Evaluation - loss: 0.797730 acc: 67.2856%(3624/5386)\n",
      "\n",
      "Step[600] - loss: 0.249587                     acc: 91.6667%(88/96))\n",
      "Evaluation - loss: 0.848579 acc: 66.4129%(3577/5386)\n",
      "\n",
      "Step[700] - loss: 0.188181                     acc: 92.7083%(89/96))\n",
      "Evaluation - loss: 0.852056 acc: 67.3784%(3629/5386)\n",
      "\n",
      "Step[800] - loss: 0.096367                     acc: 96.8750%(93/96))\n",
      "Evaluation - loss: 0.894710 acc: 66.7843%(3597/5386)\n",
      "\n",
      "Step[900] - loss: 0.126961                     acc: 95.8333%(92/96))\n",
      "Evaluation - loss: 0.923036 acc: 67.6012%(3641/5386)\n",
      "\n",
      "Step[1000] - loss: 0.135729                     acc: 95.8333%(92/96)\n",
      "Evaluation - loss: 0.955607 acc: 67.0813%(3613/5386)\n",
      "\n",
      "Step[1100] - loss: 0.078745                     acc: 98.9583%(95/96))\n",
      "Evaluation - loss: 0.992288 acc: 67.4155%(3631/5386)\n",
      "\n",
      "Step[1200] - loss: 0.067515                     acc: 96.8750%(93/96))\n",
      "Evaluation - loss: 1.022520 acc: 66.8585%(3601/5386)\n",
      "\n",
      "Step[1300] - loss: 0.050186                     acc: 100.0000%(96/96)\n",
      "Evaluation - loss: 1.042833 acc: 66.2087%(3566/5386)\n",
      "\n",
      "Step[1400] - loss: 0.039724                     acc: 98.9583%(95/96))\n",
      "Evaluation - loss: 1.058231 acc: 66.7843%(3597/5386)\n",
      "\n",
      "early stop by 1000 steps.\n"
     ]
    }
   ],
   "source": [
    "### 模型 ###\n",
    "# 模型训练\n",
    "config.embed_num = len(text_field.vocab)\n",
    "config.class_num = len(label_field.vocab) - 1\n",
    "textcnn = TextCNN(config)\n",
    "\n",
    "# 重新开始训练\n",
    "for file in os.listdir('model'):\n",
    "    if file.find('.pt')>=0:\n",
    "        os.remove('model/'+file)\n",
    "    \n",
    "try:\n",
    "    train(textcnn,train_iter,test_iter,config)\n",
    "except KeyboardInterrupt:\n",
    "    print('\\n'+'-'*88)\n",
    "    print(\"interrupted by keyboard, stop training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation - loss: 0.774350 acc: 68.5481%(3692/5386)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 选择最优模型\n",
    "models_path = list(filter(lambda x:x.find('.pt')>0, os.listdir('model')))\n",
    "models_path = list(map(lambda x:os.path.join('model',x), models_path))\n",
    "models_path = list(sorted(models_path, key=lambda x:os.path.getmtime(x)))\n",
    "best_model_path = models_path[-1]\n",
    "# 注意重载时的模型类要和之前的一样\n",
    "textcnn = TextCNN(config)\n",
    "state_dict = torch.load(best_model_path)\n",
    "textcnn.load_state_dict(state_dict)\n",
    "\n",
    "# 模型评价\n",
    "try:\n",
    "    evaluate(textcnn,test_iter)\n",
    "except:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Plz enter a sentence for prediction:\n",
      " Beam me up, Scotty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KIRK\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Plz enter a sentence for prediction:\n",
      " Sensor scan to one half parsec. Negative, Captain.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPOCK\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Plz enter a sentence for prediction:\n",
      " Change course, come about to one eight five, mark three.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KIRK\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Plz enter a sentence for prediction:\n",
      " I believed the Romulans have developed a cloaking device which renders our tracking sensors useless.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPOCK\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Plz enter a sentence for prediction:\n",
      " Jim, he's dead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCCOY\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Plz enter a sentence for prediction:\n",
      " Well, you can see for yourself, he's mentally depressed, physically weak, disoriented, displays of feelings of persecution and rebellion.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCCOY\n",
      "\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "# 模型预测->交互式\n",
    "with open('model/text_field.txt','rb') as f:\n",
    "    text_field = pickle.load(f)\n",
    "with open('model/label_field.txt','rb') as f:\n",
    "    label_field = pickle.load(f)\n",
    "while True:\n",
    "    try:\n",
    "        text = input(\"Plz enter a sentence for prediction:\\n\")\n",
    "        predict(text,textcnn,text_field,label_field)\n",
    "        print()\n",
    "    except KeyboardInterrupt:\n",
    "        print('Exiting...')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
