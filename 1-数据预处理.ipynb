{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 写在前面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做这一个系列的notebooks，是为了帮助自己好好梳理一下自然语言处理（NLP）过程中的方法和模型。\n",
    "\n",
    "一共分为以下几个部分：\n",
    "+ 数据预处理\n",
    "+ 词袋模型的使用\n",
    "+ n-gram及共现的概念\n",
    "+ 词的分布式表示\n",
    "+ torchtext的使用\n",
    "+ CNN与RNN【RNN暂缺】\n",
    "+ Attention机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整个notebook系列使用的数据是星际迷航原初（TOS）剧集中的所有台词，参见raw_data文件夹。\n",
    "\n",
    "<img src='images/startrek.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本数据预处理中有几个核心概念：\n",
    "+ **tokenize**：分词，将句子切分成一个个单词/词汇与符号\n",
    "+ **filter**：过滤，利用停词表（stopwords）将一些非常常见但是没有意义的词去掉，如is/the/to...\n",
    "+ **POS tagging**：词性标注，将词划分为名次/动词/形容词...\n",
    "+ **stemming/lemmatisation**：词干提取/词形还原，将词还原成最原始的状态，如stopped->stop\n",
    "\n",
    "以上所说的方法并非预处理步骤中所必须的，应该根据具体问题来进行选择。\n",
    "\n",
    "下面会展示[**nltk**](http://www.nltk.org/)和[**jieba**](https://github.com/fxsjy/jieba)中各个方法的具体使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import jieba\n",
    "import jieba.analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词函数可以自己定义，也可以使用现成的->很多NLP的库都会提供tokenizer或相应机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_sents: ['Maybe we can’t stroll to the music of the lute.', 'We must march to the sound of drums.']\n",
      "en_words: ['Maybe', 'we', 'can', '’', 't', 'stroll', 'to', 'the', 'music', 'of', 'the', 'lute', '.', 'We', 'must', 'march', 'to', 'the', 'sound', 'of', 'drums', '.']\n",
      "ch_words: ['选择', '洗衣机', '、', '汽车', '、', '镭射', '音响', '，', '还有', '电动', '开罐器', '。']\n"
     ]
    }
   ],
   "source": [
    "# 自定义\n",
    "def tokenize(text):\n",
    "    text = text.lower() #将字符串全部降为小写\n",
    "    text = re.split(r',|\\.|\\?|!|\"|\\'| |\\n',text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# nltk\n",
    "en = \"Maybe we can’t stroll to the music of the lute. We must march to the sound of drums.\"\n",
    "en_sents = nltk.sent_tokenize(en) #利用“./?/!”对文本段进行分句\n",
    "en_words = nltk.word_tokenize(en) #利用所有的标点符号对文本进行分词\n",
    "\n",
    "# jieba\n",
    "ch = \"选择洗衣机、汽车、镭射音响，还有电动开罐器。\"\n",
    "ch_words = list(jieba.cut(ch))\n",
    "\n",
    "print(\"en_sents:\",en_sents)\n",
    "print(\"en_words:\",en_words)\n",
    "print(\"ch_words:\",ch_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于中文的分词较为复杂，可以加载自定义的词典来绑定词组，使用`jieba.load_userdict(file_name)`将准备好的txt格式词典载入，每行一个词语。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_stopwords: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "# nltk\n",
    "# from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english') #得到的是一个list\n",
    "print(\"en_stopwords:\",stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到nltk提供的停词表词汇量其实很少，往往无法满足深度过滤的需求。可以用append自行添加词汇。\n",
    "\n",
    "而jieba则没有提供stopwords，需要自己定义，source文件夹中包含一份中文停词stopwords.txt。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Maybe', '’', 'stroll', 'music', 'lute', '.', 'We', 'must', 'march', 'sound', 'drums', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_words = []\n",
    "for word in en_words:\n",
    "    if word not in stopwords:\n",
    "        filtered_words.append(word)\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词性标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时候我们只关注名词、动词或形容词（比如在提取评论关键词时），这就需要对词的词性进行区分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stroll NN\n",
      "music NN\n",
      "lute NN\n",
      "march VB\n",
      "sound NN\n",
      "drums NNS\n",
      "\n",
      "选择 v\n",
      "洗衣机 n\n",
      "汽车 n\n",
      "音响 n\n",
      "还有 v\n",
      "电动 n\n",
      "开罐器 n\n"
     ]
    }
   ],
   "source": [
    "# nltk\n",
    "for word,pos in nltk.pos_tag(filtered_words): #传参为list\n",
    "    if pos in ('NN','NNS','VB'):\n",
    "        print(word,pos)\n",
    "print()\n",
    "# jieba\n",
    "for word,pos in jieba.posseg.cut(ch): #传参为str\n",
    "    if pos in ('v','n'):\n",
    "        print(word,pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词型还原/词干提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两个概念仅涉及西方语系语种，和中文不搭边。nltk中两种处理手段的工具都有提供。\n",
    "\n",
    "从体验上来看，`词干提取`风险较大，经常让单词缺胳膊少腿，对不规则变化也无法识别。而`词性还原`本质是基于字典，虽然准确度可以有所保证，但加载比较慢，而且还需要指定词性。\n",
    "\n",
    "总的来说，使用起来都不太理想。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiking: hike hike\n",
      "ran: ran run\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "print('hiking:',stemmer.stem('hiking'),lemmatizer.lemmatize('hiking','v'))\n",
    "print('ran:',stemmer.stem('ran'),lemmatizer.lemmatize('ran','v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy\n",
    "基于以上的种种不方便，在此提供另一个库[SpaCy](https://spacy.io/)。它用一个模型解决所有分词、词型还原、词性标注、停词信息等问题。\n",
    "\n",
    "（不过随着文本长度增加，其处理速度会显著变慢）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maybe maybe ADV RB advmod Xxxxx True False\n",
      "we -PRON- PRON PRP nsubj xx True True\n",
      "ca can VERB MD aux xx True True\n",
      "n’t not PART RB neg x’x False True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") #需下载\n",
    "doc = nlp(en)\n",
    "\n",
    "for token in doc[:4]:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
