{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - 词袋模型\n",
    "**词袋**(Bag-of-words)是将一个文档看作一个袋子，词就是袋子中的物品，不考虑其出现的先后顺序，只考察是否出现以及出现的频数。\n",
    "\n",
    "而进一步，引入[tf-idf]()的概念，将每个文档中的词的频数替换为其tf-idf值，则可以选取文档中最具有代表性的词。\n",
    "\n",
    "通过建立词袋模型，我们可以得到：\n",
    "+ 所有词的**look-up table**（储存索引的字典），{'this':101,'is':3,'illogical':3421...}\n",
    "+ 每个文档的**词袋表示**：doc1-> ((101,2), (3,5), (3421,2),...)\n",
    "+ 用于计算每个词tf-idf值的**模型**\n",
    "+ 每个文档的**tf-idf表示**：doc1-> ((101,0.4233), (3,0.5644), (3421,0.8119)...)\n",
    "\n",
    "将文档用tf-idf词袋表示后，对其中的词进行**排序**，提取关键信息。\n",
    "\n",
    "接下来使用[gensim](https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py)实现建模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "docs = []\n",
    "# 读取数据\n",
    "for root,dirs,files in os.walk('raw_data/'):\n",
    "    for file in files:\n",
    "        if file.find('.txt')<0: continue\n",
    "        with open(os.path.join(root,file)) as f:\n",
    "            season = root.split('/')[-1]\n",
    "            episode = re.search(r'^\\d+',file).group(0)\n",
    "            titles.append('{}_{}'.format(season,episode))\n",
    "            docs.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "nlp = spacy.load('en') #借助spcay\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text.lower())\n",
    "    words = []\n",
    "    for token in doc:\n",
    "        # 去除符号、停词、长度为一的字符，并进行词形还原\n",
    "        if token.pos_!='PUNCT' and not token.is_stop and len(set(token.text))>1:\n",
    "            words.append(token.lemma_)\n",
    "    return words\n",
    "\n",
    "def prep_data(doc): \n",
    "    # 在分词前对原始台词数据进行初步处理\n",
    "    doc = doc.split('\\n')\n",
    "    lines = ['']\n",
    "    for l in doc:\n",
    "        if l.find('[')>0 or l.find(']')>0: #去除场景说明\n",
    "            continue\n",
    "        \n",
    "        if l.find(':')>0: #去除每句台词前的角色说明\n",
    "            l = l[l.index(':')+2:]\n",
    "            lines.append(l)\n",
    "        else:\n",
    "            lines[-1] += ' '+l #将一句台词连成一个句子\n",
    "    \n",
    "    return preprocess(' '.join(lines))\n",
    "\n",
    "\n",
    "docs = list(map(prep_data,docs))\n",
    "processed_corpus = np.array(docs).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始构建**词袋模型**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核心元素：\n",
    "+ dictionary = `corpora.Dictionary`(processed_corpus)。将所有文档切分至词，一起用于构建字典\n",
    "+ doc_bow = `dictionary.doc2bow`(doc)。输入分词后的文档得到其词袋表示\n",
    "+ tfidf_model = `models.TfidfModel`(bow_corpus)。利用所有文档的词袋表示训练tf-idf模型\n",
    "+ doc_tfidf = `tfidf[doc_bow]`。利用模型将词袋表示转化为tfidf表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "tfidf_model = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words num: 11655\n",
      "\n",
      "dictionary:\n",
      " [('achieve', 10), ('acknowledge', 11), ('activate', 12), ('actually', 13), ('adc', 14), ('additional', 15), ('address', 16), ('affectionate', 17), ('affirmative', 18), ('afraid', 19)]\n"
     ]
    }
   ],
   "source": [
    "# 获取字典\n",
    "stoi = dictionary.token2id\n",
    "print(\"words num:\",len(stoi))\n",
    "print(\"\\ndictionary:\\n\",list(stoi.items())[10:20]) \n",
    "#没有id2token方法，需要用zip(dict.values(),dict.keys())自定义\n",
    "itos = dict(zip(stoi.values(),stoi.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取feature words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_bow:\n",
      " [(10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]\n",
      "\n",
      "doc_tfidf:\n",
      " [(10, 0.022499206359733896), (11, 0.00729610721246217), (12, 0.006375927830181293), (13, 0.00791776395883734), (14, 0.03010193465712729), (15, 0.013894232750314374), (16, 0.014461760895031426), (17, 0.03010193465712729), (18, 0.00937320863819255), (19, 0.003396739846715514)]\n"
     ]
    }
   ],
   "source": [
    "# 得到词袋表示\n",
    "doc_bow = dictionary.doc2bow(docs[0])\n",
    "print(\"doc_bow:\\n\",doc_bow[10:20])\n",
    "\n",
    "# 得到tf-idf表示\n",
    "doc_tfidf = tfidf_model[doc_bow]\n",
    "print(\"\\ndoc_tfidf:\\n\",doc_tfidf[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_words: ['christopher', 'ufo', 'colonel', 'aircraft', 'bluejay', 'sergeant', 'photo', 'contribution', 'chronometer', 'breakaway', 'tractor', 'son', 'airman', 'blackjack', 'cygnet', 'fighter', 'geoffrey', 'hapless', 'retrain', 'shaun']\n"
     ]
    }
   ],
   "source": [
    "# 根据tf-idf值进行排序\n",
    "sorted_tfidf = list(sorted(doc_tfidf,key=lambda x:x[1],reverse=True))\n",
    "top_words = [itos[x[0]] for x in sorted_tfidf[:20]]\n",
    "print(\"top_words:\",top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时候可以在`构建词典时`使用`filter_extremes`方法，进一步对不具备代表性的高频词进行过滤。\n",
    "\n",
    "**注意**：这个方法对整个字典的影响非常显著，甚至彻底改变top_words的输出。要反复尝试选取最佳参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words num: 5439\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "new_dict = deepcopy(dictionary) # 因为filter_extremes是inplace方法，所以还是复制一份保险\n",
    "# 排除在所有文档中出现次数不超过5次和出现文档占比超过50%的词\n",
    "new_dict.filter_extremes(no_below=2,no_above=0.3) \n",
    "new_stoi = new_dict.token2id\n",
    "print(\"words num:\",len(new_stoi))\n",
    "new_itos = dict(zip(new_stoi.values(),new_stoi.keys()))\n",
    "\n",
    "new_bow_corpus = [new_dict.doc2bow(doc) for doc in docs]\n",
    "new_tfidf_model = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words: ['colonel', 'chronometer', 'sun', 'accident', 'son', 'backward', 'gain', 'aircraft', 'sergeant', 'noise', 'achieve', 'crush', 'film', 'passenger', 'black', 'current', 'christopher', 'auxiliary', 'bag', 'gun']\n"
     ]
    }
   ],
   "source": [
    "new_doc_bow = new_dict.doc2bow(docs[0])\n",
    "new_doc_tfidf = new_tfidf_model[new_doc_bow]\n",
    "\n",
    "new_sorted_tfidf = list(sorted(new_doc_tfidf,key=lambda x:x[1],reverse=True))\n",
    "new_top_words = [new_itos[x[0]] for x in new_sorted_tfidf[:20]]\n",
    "print(\"top words:\",new_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "我觉得词袋模型并不是一个主模型，而更像是数据预处理的尾巴。通过建立词袋提取关键信息，或是将词袋模型作为接下来的输入。我觉得学到概念就行。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
